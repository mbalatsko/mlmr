{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMR sklearn transformer integration\n",
    "\n",
    "We have also added posibility to make use of MapReduce functionality as sklean transformer. This functionality is just a fancy wrap that uses core functionality from `mlmr.function` module. If you haven't read `Functional tutorial` yet, I'd recommend you to check it out before reading this one.\n",
    "\n",
    "As a starting point let's assume that we are solving text classification problem and we need to integrate parallelized text lemmatization function into our sklearn pipeline. Let's take code of text lemmatization from the `Functional tutorial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "texts = pd.Series([\n",
    "    \"\"\"\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
    "    Ut maximus consequat turpis et condimentum. \n",
    "    Duis ullamcorper dictum posuere.\n",
    "    Curabitur auctor quis sapien congue aliquet. \n",
    "    Aliquam dignissim suscipit rhoncus. \n",
    "    Fusce vitae cursus dui, eu aliquam dui. \n",
    "    Nulla et ultrices lacus, at iaculis arcu. \n",
    "    Sed fermentum metus libero, sed egestas libero ultrices sed. \n",
    "    Duis erat leo, ultricies quis dapibus non, lacinia ut tellus.\n",
    "    \"\"\"\n",
    "]*10000)\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def preprocess_texts_df(df): # our transform(map) function\n",
    "    return df.apply(preprocess_text)\n",
    "\n",
    "def remove_punct(doc):\n",
    "    return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "def remove_stop_words(doc):\n",
    "    return [t for t in doc if not t.is_stop]\n",
    "\n",
    "def lemmatize(doc):\n",
    "    return ' '.join([t.lemma_ for t in doc])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    removed_punct = remove_punct(doc)\n",
    "    removed_stop_words = remove_stop_words(removed_punct)\n",
    "    return lemmatize(removed_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared two kind of transformers you could use:\n",
    "\n",
    "`mlmr.transformers.BaseMapReduceTransformer`, which is a sklearn wrapper base class for `mlmr.function.transform_concat` function. Just to refresh your memory `mlmr.function.transform_concat` description:\n",
    "\n",
    ">Function for performing parallel data transformations on data (pd.DataFrame, pd.Series). From n_jobs argument, number of processes to run in parallel is calculated. Data is evenly divided into number of processes slices. Then transform_func is applied on each slice in parallel. After calculation is complete all transformation results are flattened. Flattened result is returned. Data preserves initial ordering.\n",
    "\n",
    "You have to create a child class, which implements `transform_part` function. This function stands for `transform_func` in a functional interface and for **Map** part in map reduce. So your implementation could be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmr.transformers import BaseMapReduceTransformer\n",
    "    \n",
    "class TextPreprocessor(BaseMapReduceTransformer):\n",
    "    \n",
    "    def transform_part(self, X):\n",
    "        return preprocess_texts_df(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can easily integrate it, into your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_jobs = 4\n",
    "\n",
    "text_classification_pipeline = Pipeline([\n",
    "     ('text_preprocessor', TextPreprocessor(n_jobs=n_jobs)),\n",
    "     ('vectorizer', TfidfVectorizer(analyzer = \"word\", max_features=10000)),\n",
    "     ('classifier', RandomForestClassifier(n_estimators=100, n_jobs=n_jobs))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we have prepared a transformer similar to `sklearn.FunctionTransformer`:\n",
    "`mlmr.transformers.FunctionMapReduceTransformer`, which is a sklearn wrapper class for `mlmr.function.map_reduce` function. Just to refresh your memory `mlmr.function.map_reduce` description:\n",
    "\n",
    ">Base function for performing parallel MapReduce on data. Firstly data are splitted into data splits using `data_split_func` function. From `n_jobs` argument, number of processes to run in parallel is calculated. Then `map_func` is applied on each data split in parallel. After calculation is complete `reduce_func` is sequentially applied on list of `map_func` results. `reduce_func` result is returned. Data preserves initial ordering.\n",
    "\n",
    "So basically it is a base function using which you can formulate any MapReduce problem. Usage of this object could look as follows (this representation is equivalent to previous example):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlmr.transformers import FunctionMapReduceTransformer\n",
    "\n",
    "def get_split_data_func(n_slices): # wrapper function of split data function\n",
    "    def split_data(data):\n",
    "        return np.array_split(data, n_slices)\n",
    "    return split_data\n",
    "\n",
    "n_jobs = 4\n",
    "\n",
    "text_classification_pipeline = Pipeline([\n",
    "     ('text_preprocessor', FunctionMapReduceTransformer(\n",
    "         map_func=preprocess_texts_df,\n",
    "         reduce_func=pd.concat,\n",
    "         data_split_func=get_split_data_func(n_jobs),\n",
    "         n_jobs=n_jobs\n",
    "     )),\n",
    "     ('vectorizer', TfidfVectorizer(analyzer = \"word\", max_features=10000)),\n",
    "     ('classifier', RandomForestClassifier(n_estimators=100, n_jobs=n_jobs))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope it will help you parallelize your ML projects! If you have found any error or if you have got an idea of an improvement, post it in GitHub issues and we'll work on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
