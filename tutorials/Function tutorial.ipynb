{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLMR tutorial\n",
    "\n",
    "This library will help you easily parallelize your python code for all kind of data transformations. Core functions are built on Map-Reduce paradigm. In this library **Map** part is parallelized using native python `multiprocessing` module. Let's define what Map-Reduce paradigm is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install mlmr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "Wikipedia says:\n",
    "> MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.\n",
    "\n",
    "Looks like something complex, like something you could use only on a big cluster or on an enormous dataset, right? But it is very easy, let's explain it in a couple of sentences and 1 example.\n",
    "MapReduce is divided into two parts:\n",
    "Map — function, which somehow transforms an input into an output. This function is called on every sample or sequence of samples of data, which have to be processed. This step could be easily parallelized, because for each sample or sequence of samples, in fact, you do the same operation.\n",
    "Reduce — function, that aggregates all output of the map function. This step is sequential.\n",
    "\n",
    "## Sum of squares example\n",
    "For example, you have to calculate the sum of squares of an array of data. Sequential implementation would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "arr = [1, 2, 3, 4, 5]\n",
    "result = 0\n",
    "for i in arr:\n",
    "    result += i**2\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s convert this task into MapReduce:\n",
    "1. Map — calculate the square of the number\n",
    "1. Reduce — calculate the sum of map results\n",
    "\n",
    "Sequential implementation in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "map_arr = map(lambda x: x**2, arr)\n",
    "result = reduce(lambda x, y: x + y, map_arr)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce task definition\n",
    "So now I hope you have brief understanding of the MapReduce concept, and we can move to what this library offers. Actually for any MapReduce task you have to define:\n",
    "1. **Map function**\n",
    "1. **Map function input** - you have to define how you data would be splited into parts, which would be fed to map function in parallel. Most obvious data splitting methods are data split into single samples or split data into N equal size splits.\n",
    "1. **Data split function** - function which will turn data into list of map outputs.\n",
    "1. **Reduce function**\n",
    "1. **Reduce function input** - actually reduce output is list of map outputs\n",
    "\n",
    "Hand's up! Let's show you usage of library functions on real examples. Let's use the example from above section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlmr.function API interface definition on examples:\n",
    "```python\n",
    "def map_reduce(\n",
    "    data,\n",
    "    data_split_func,\n",
    "    map_func,\n",
    "    reduce_func,\n",
    "    n_jobs\n",
    "):\n",
    "\n",
    "```\n",
    "Base function for performing parallel MapReduce on data. Firstly data are splitted into data splits using `data_split_func` function. From `n_jobs` argument, number of processes to run in parallel is calculated. Then `map_func` is applied on each data split in parallel. After calculation is complete `reduce_func` is sequentially applied on list of `map_func` results. `reduce_func` result is returned. Data preserves initial ordering.\n",
    "\n",
    "**Arguments:**\n",
    "1. `data` (*Iterable*) - data on which MapReduce would be performed.\n",
    "2. `data_split_func` (*Callable*) - function that would be used to split data to perform Map operation. Data split function signature: *func(Iterable) -> Iterable\\[Any\\]*\n",
    "3. `map_func` (*Callable*) - Map function. Map function signature: *func(DataSplit) -> Any*. **Function can't be lambda or local function!**\n",
    "4. `reduce_func` - (*Callable*) - Reduce function. Reduce function signature: *func(Iterable\\[MapResult\\]) -> Any*.\n",
    "5. `n_jobs` - number of jobs to run in parallel. `-1` means using all processors.\n",
    "\n",
    "**Returns**: Transformed (MapReduced) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to `Sum of squares task`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "from mlmr.function import map_reduce\n",
    "\n",
    "def square(x): # our map function\n",
    "    return x**2\n",
    "\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "\n",
    "split_data = lambda x: x # identity function, because we want to split it by element\n",
    "\n",
    "result = map_reduce(\n",
    "    data=arr,\n",
    "    data_split_func=split_data,\n",
    "    map_func=square,\n",
    "    reduce_func=sum, # python builtin function\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can define alternative splitting function. Let's say that we want each process to calculate `sum of squares` on its slice of data. So that we will dicrease the processes communication latency. \n",
    "\n",
    "This change could be actually done in a several ways:\n",
    "\n",
    "**Map** will calculate square of each number and return list of squares. **Reduce** will flatten **Map** results and calculate sum over all numbers (Could be slower version, because sequential part performs more work) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mlmr.function import map_reduce\n",
    "\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "\n",
    "def squares_of_slice(arr_slice): # our map function\n",
    "    return list(map(lambda x: x**2, arr_slice))\n",
    "\n",
    "def flatten_sum(map_results): # our reduce function\n",
    "    return sum([item for sublist in map_results for item in sublist])\n",
    "\n",
    "def get_split_data_func(n_slices): # wrapper function of split data function\n",
    "    def split_data(data):\n",
    "        return np.array_split(data, n_slices)\n",
    "    return split_data\n",
    "\n",
    "n_jobs = 2\n",
    "\n",
    "result = map_reduce(\n",
    "    data=arr,\n",
    "    data_split_func=get_split_data_func(n_jobs), # split data into n_jobs slices\n",
    "    map_func=squares_of_slice,\n",
    "    reduce_func=flatten_sum,\n",
    "    n_jobs=n_jobs\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map** will calculate square of each number and their sum (actually perform partial reduction). **Reduce** will have the same input, so will not be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mlmr.function import map_reduce\n",
    "\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "\n",
    "def squares_of_slice(arr_slice): # our map function, with partial reduction\n",
    "    return sum(map(lambda x: x**2, arr_slice))\n",
    "\n",
    "def get_split_data_func(n_slices): # wrapper function of split data function\n",
    "    def split_data(data):\n",
    "        return np.array_split(data, n_slices)\n",
    "    return split_data\n",
    "\n",
    "n_jobs = 2\n",
    "\n",
    "result = map_reduce(\n",
    "    data=arr,\n",
    "    data_split_func=get_split_data_func(n_jobs), # split data into n_jobs slices\n",
    "    map_func=squares_of_slice,\n",
    "    reduce_func=sum,\n",
    "    n_jobs=n_jobs\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def map_reduce_splits(\n",
    "    data_splits,\n",
    "    map_func,\n",
    "    reduce_func,\n",
    "    n_jobs\n",
    "):\n",
    "\n",
    "```\n",
    "Base function for performing parallel MapReduce on `data_splits`. From `n_jobs` argument, number of processes to run in parallel is calculated. Then `map_func` is applied on each element of `data_splits` in parallel. After calculation is complete `reduce_func` is sequentially applied on list of `map_func` results. `reduce_func` result is returned. Data preserves initial ordering.\n",
    "\n",
    "**Arguments:**\n",
    "1. `data_splits` (*Iterable*) - data splits on which MapReduce would be performed.\n",
    "3. `map_func` (*Callable*) - Map function. Map function signature: *func(DataSplit) -> Any*. **Function can't be lambda or local function!**\n",
    "4. `reduce_func` - (*Callable*) - Reduce function. Reduce function signature: *func(Iterable\\[MapResult\\]) -> Any*.\n",
    "5. `n_jobs` - number of jobs to run in parallel. `-1` means using all processors.\n",
    "\n",
    "**Returns**: Transformed (MapReduced) data splits.\n",
    "\n",
    "This function is just a variation of  the previous one. So we can a little bit rewrite code to be able to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "from mlmr.function import map_reduce_splits\n",
    "\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "\n",
    "def squares_of_slice(arr_slice): # our map function, with partial reduction\n",
    "    return sum(map(lambda x: x**2, arr_slice))\n",
    "\n",
    "def get_split_data_func(n_slices): # wrapper function of split data function\n",
    "    def split_data(data):\n",
    "        return np.array_split(data, n_slices)\n",
    "    return split_data\n",
    "\n",
    "n_jobs = 2\n",
    "\n",
    "data_slices = get_split_data_func(n_jobs)(arr)\n",
    "\n",
    "result = map_reduce_splits(\n",
    "    data_splits=data_slices,\n",
    "    map_func=squares_of_slice,\n",
    "    reduce_func=sum,\n",
    "    n_jobs=n_jobs\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation parallelization for basic ML stack (pandas+sklean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def transform_concat(\n",
    "    data: Iterable,\n",
    "    transform_func: Callable[[Iterable], Any],\n",
    "    n_jobs: int = 1\n",
    "):\n",
    "\n",
    "```\n",
    "Function for performing parallel data transformations on `data` (pd.DataFrame, pd.Series). From `n_jobs` argument, number of processes to run in parallel is calculated. Data is evenly divided into number of processes slices. Then `transform_func` is applied on each slice in parallel. After calculation is complete all transformation results are flattened. Flattened result is returned. Data preserves initial ordering.\n",
    "\n",
    "**Arguments:**\n",
    "1. `data` (*Iterable*) - data on which transformation using MapReduce would be performed.\n",
    "3. `transform_func` (*Callable*) - transformation function of a `data` **split** . Transform function signature: *func(Union\\[pd.DataFrame, pd.Series\\]) -> Union\\[pd.DataFrame, pd.Series\\]*. **Function can't be lambda or local function!**\n",
    "5. `n_jobs` - number of jobs to run in parallel. `-1` means using all processors.\n",
    "\n",
    "**Returns**: Transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually here I want to stop you attention. During data exploration I often suffer from slow data transformation execution. Sometimes it takes ten's of minutes to get the result of a single transformation. I was surfing web actually for a long time and haven't found good solution haow to parallelize usual ml stack transformations not quiting using pandas (of course you can you dask and stuff, but some times it is just too much). So I have prepared an easy solution for you to use, without making a big effort. \n",
    "\n",
    "I'll show you an example of parallelization `pd.Series.apply` method using `mlmr.function.transform_concat`, that I have used in my old project. The task is transparent in this case, common NLP pipeline of text lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datset size: 50000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "texts = pd.Series([\n",
    "    \"\"\"\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
    "    Ut maximus consequat turpis et condimentum. \n",
    "    Duis ullamcorper dictum posuere.\n",
    "    Curabitur auctor quis sapien congue aliquet. \n",
    "    Aliquam dignissim suscipit rhoncus. \n",
    "    Fusce vitae cursus dui, eu aliquam dui. \n",
    "    Nulla et ultrices lacus, at iaculis arcu. \n",
    "    Sed fermentum metus libero, sed egestas libero ultrices sed. \n",
    "    Duis erat leo, ultricies quis dapibus non, lacinia ut tellus.\n",
    "    \"\"\"\n",
    "]*50000)\n",
    "print('Datset size:', len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def preprocess_texts_df(df): # our transform(map) function\n",
    "    return df.apply(preprocess_text)\n",
    "\n",
    "def remove_punct(doc):\n",
    "    return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "def remove_stop_words(doc):\n",
    "    return [t for t in doc if not t.is_stop]\n",
    "\n",
    "def lemmatize(doc):\n",
    "    return ' '.join([t.lemma_ for t in doc])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    removed_punct = remove_punct(doc)\n",
    "    removed_stop_words = remove_stop_words(removed_punct)\n",
    "    return lemmatize(removed_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmr.function import transform_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example I would like you to show the execution speed up. You can see that until using 8 processes the speed up is almost linear and the it start to stognate. Well you can't speed up to the infinity by just adding working processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 36s, sys: 31.7 ms, total: 8min 36s\n",
      "Wall time: 8min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "texts_preprocessed = preprocess_texts_df(texts) # sequential implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 ms, sys: 72.1 ms, total: 245 ms\n",
      "Wall time: 5min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "texts_preprocessed = transform_concat(texts, preprocess_texts_df, n_jobs=2) # using 2 processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 140 ms, sys: 64.1 ms, total: 204 ms\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "texts_preprocessed = transform_concat(texts, preprocess_texts_df, n_jobs=4) # using 4 processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 92.1 ms, total: 244 ms\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "texts_preprocessed = transform_concat(texts, preprocess_texts_df, n_jobs=8) # using 8 processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 192 ms, sys: 132 ms, total: 324 ms\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "texts_preprocessed = transform_concat(texts, preprocess_texts_df, n_jobs=12) # using 12 processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
